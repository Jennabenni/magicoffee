- What did you build? Describe the feature in plain language.

- How did micro-iteration feel? Was working in small steps natural or frustrating? Why?

- What did self-review catch? When you asked the AI to review its own code, what issues did it find? Give at least one specific example.

- Tool impressions. What did you like or dislike about [Copilot Agent / Claude Web]?



- Self-review patterns. Did the AI consistently catch certain types of issues during self-review (e.g., edge cases, missing error handling)? Did it ever miss something you caught yourself?


- Browser tool vs. CLI comparison. If you’ve used Claude Code CLI or another terminal tool, how did the browser-based experience compare? What’s better/worse about each?


- When would you use micro-iteration + self-review? For what kinds of tasks does this workflow make sense? When would you skip it?



What I built with Copilot was a drink quiz above the menu that allows the user to answer some questions and be suggested a drink from our menu, as well as some flavor add-ons.  There were four results that were possible, two drinks from the hot menu, and two from the cold menu.  The micro-iteration felt normal and helped me pace myself, and helped the AI as well reference what we were doing with each step, so I wouldn't have to repeat myself.  I had to suggest a different order at one point, however, because I was taught to do CSS at the end, because we should make things work first, and then make them pretty, but the AI had this step after the skeleton part of the quiz, before there were any questions.  In the self-review, the only thing the AI caught was a missing semi-colon in the menu file.  It stated that all other features worked, and I verified for myself and confirmed that the quiz worked as intended.  The semi colon was missing on a className on line 285 in menu.html.  I liked that Copilot seemed responsive to what I was asking and only edited when requested.  With Copilot however, there were quite a few issues I had that I did not ever have with the Claude CLI.  When it made the quiz, it was really struggling to make the answer and the radio button look nice, and I eventually had to tell it to change the radio buttons into actual buttons to be an easier block to manipulate.  I also originally was trying to make it give eight different results, since I have eight different drink items, however, asking for that one command ruined the entirety of the code for menu.html.  It somehow got rid of all of the html on the page and replaced it with script only, and then the script wasn't even properly coded or commented out, so the page looked like a jumbled reading of script.  I had to delete everything and take my previous code from Github (I hadn't pushed anything that the AI did) and just ask it for four instead, since my tokens were getting close and I didn't want it to screw up anything else.  I've used Copilot before, but I actually prefer the non-student version, because it feels a bit invasive when it's editing my code, and while I reviewed its edits, I trusted it too much towards the end.


The AI missed a lot of issues that it caused, and I had to correct it quite a few times.  It messed up with the CSS mostly in the beginning, so I had to course correct with my own knowledge.  I wanted to go in and just do it myself, but I didn't want to compromise the assignment.  It also messed up towards the end as I previously mentioned, and said that it was all done making eight possible endings, yet it completely destroyed my code and made it unusable.  Thankfully I had the old code, or else I would've had to start from the beginning.  It also only had one outcome for the quiz initially, and every result was 'capuccino', despite me asking for more options.  It didn't seem to understand some of the things I was asking, despite me being clear.  I definitely believe the terminal-based Claude is much better than this Copilot agent.  With Claude, I actually hadn't had any substantial mistakes or errors, and it seems incredibly intelligent for an AI program.  Copilot definitely seems below it, and made countless errors that I had to catch.  The upside with Claude is that it predicts what you want and understands outside logic, and can therefore infer what would be on each file while making it, and add additional features that you didn't mention.  I recall when I was coding the Magicoffee website, I wanted the bare bones of the website to start, but I didn't tell Claude that, and it did it for me, pictures and all.  The one downside could relate to that, where it could be the one guiding you if you have less experience, and may be better for more experienced coders.  With Copilot, it seems better at making conversation, but not good at editing.  Claude also explains a bit of what it's doing while it's doing it, and Copilot's thoughts span by too quickly.  While Copilot explains what it did, it is much more prone to mistakes, and needs the user to understand what they are doing.  The user should understand in both cases, but Copilot is better for intermediate-advanced coders, and it seems Copilot should be used for catching things, not for writing code on its own.  For bigger projects, micro-iteration seems like the best course of action.  Giving paragraphs of explanations may get your point across and could work from human to human, the AI would possibly get confused, and work on it step-by-step as it reads it anyway.  Additionally, the developer can check as they are going, instead of having to investigate the massive website the AI created and try and make sense of it, while also looking for errors.  Smaller amounts of code at a time can ensure that the debugging process is shortened and that the workflow is consistent.  I think micro-iteration can be skipped for smaller projects or tools, since the tool may not have many features and need a lengthy step-by-step.
